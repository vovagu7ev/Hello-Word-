{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c7480fc",
      "metadata": {
        "id": "3c7480fc"
      },
      "source": [
        "## HW4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcd6dd1b",
      "metadata": {
        "id": "fcd6dd1b"
      },
      "source": [
        "## Q1: the TD($\\lambda$) algorithm (5%)\n",
        "Consider a larger version of the Random Walk problem. \n",
        "Now we have 19 states instead of 5 and the left terminal state gives a reward of -1. \n",
        "The starting state is still the centre state. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "620d4e33",
      "metadata": {
        "id": "620d4e33"
      },
      "source": [
        "**Task**:\n",
        "- Implement the off-line $\\lambda$-return / TD($\\lambda$) algorithm:\n",
        "- Compare the RMS(root mean square) error averaged over the 19 states between the predictions at the end of the episode for the 19\n",
        "states and their true values, over the first 10 episodes, and over 10 runs (10 different sequence of walks). In other words, for each run, you generate 10 episodes and use TD($\\lambda$) to learn the value. Then you calculate the difference between the learned value and the true value (you may want to use the Bellman equation to obtain the true value), square the difference, average it over 10 runs and take the square root. \n",
        "\n",
        "- Plot the averaged RMS error vs $\\alpha$ values for the following $\\lambda$-values:\n",
        "\n",
        "parameter settings: \n",
        "```\n",
        "lambdas = [0.0, 0.4, 0.8, 0.9, 0.95, 0.975, 0.99, 1]\n",
        "alphas = [np.arange(0, 1.1, 0.1),\n",
        "          np.arange(0, 1.1, 0.1),\n",
        "          np.arange(0, 1.1, 0.1),\n",
        "          np.arange(0, 1.1, 0.1),\n",
        "          np.arange(0, 1.1, 0.1),\n",
        "          np.arange(0, 0.55, 0.05),\n",
        "          np.arange(0, 0.22, 0.02),\n",
        "          np.arange(0, 0.11, 0.01)]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class HelperFunc:\n",
        "    def __init__(self, rate, step_size):\n",
        "        self.rate = rate\n",
        "        self.step_size = step_size\n",
        "        self.weights = np.zeros(N_STATES + 2)\n",
        "\n",
        "    def value(self, state):\n",
        "        return self.weights[state]\n",
        "\n",
        "    def learn(self, state, reward):\n",
        "        pass\n",
        "\n",
        "    def new_episode(self):\n",
        "        pass\n",
        "\n",
        "class TDLambda(HelperFunc):\n",
        "    def __init__(self, rate, step_size):\n",
        "        super().__init__(rate, step_size)\n",
        "        self.rate_truncate = 1e-3\n",
        "\n",
        "    def new_episode(self):\n",
        "        self.trajectory = [START_STATE]\n",
        "        self.reward = 0.0\n",
        "\n",
        "    def learn(self, state, reward):\n",
        "        self.trajectory.append(state)\n",
        "\n",
        "        if state in END_STATES:\n",
        "            self.reward = reward\n",
        "            self.T = len(self.trajectory) - 1\n",
        "            self.off_line_learn()\n",
        "\n",
        "    def n_step_return_from_time(self, n, time):\n",
        "        end_time = min(time + n, self.T)\n",
        "        returns = self.value(self.trajectory[end_time])\n",
        "        if end_time == self.T:\n",
        "            returns += self.reward\n",
        "        return returns\n",
        "\n",
        "    def lambda_return_from_time(self, time):\n",
        "        returns = 0.0\n",
        "        lambda_power = 1\n",
        "        for n in range(1, self.T - time):\n",
        "            returns += lambda_power * self.n_step_return_from_time(n, time)\n",
        "            lambda_power *= self.rate\n",
        "            if lambda_power < self.rate_truncate:\n",
        "                break\n",
        "        returns *= 1 - self.rate\n",
        "        if lambda_power >= self.rate_truncate:\n",
        "            returns += lambda_power * self.reward\n",
        "        return returns\n",
        "\n",
        "    def off_line_learn(self):\n",
        "        for time in range(self.T):\n",
        "            state = self.trajectory[time]\n",
        "            delta = self.lambda_return_from_time(time) - self.value(state)\n",
        "            delta *= self.step_size\n",
        "            self.weights[state] += delta\n",
        "\n",
        "def random_walk(helper_func):\n",
        "    helper_func.new_episode()\n",
        "    current_state = START_STATE\n",
        "    while current_state not in END_STATES:\n",
        "        new_state = current_state + np.random.choice([-1, 1])\n",
        "        if new_state == 0:\n",
        "            reward = -1\n",
        "        elif new_state == N_STATES + 1:\n",
        "            reward = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "        helper_func.learn(new_state, reward)\n",
        "        current_state = new_state\n",
        "\n",
        "# constants\n",
        "N_STATES = 19\n",
        "states = np.arange(1, N_STATES + 1)\n",
        "\n",
        "START_STATE = 10\n",
        "END_STATES = [0, N_STATES + 1]\n",
        "\n",
        "real_state_values = np.arange(-20, 22, 2) / 20.0\n",
        "real_state_values[0] = real_state_values[N_STATES + 1] = 0.0\n",
        "\n",
        "runs = 56\n",
        "lambdas = [0.0, 0.4, 0.8, 0.9, 0.95, 0.975, 0.99, 1]\n",
        "alphas = [np.arange(0, 1.1, 0.1),\n",
        "          np.arange(0, 1.1, 0.1),\n",
        "          np.arange(0, 1.1, 0.1),\n",
        "          np.arange(0, 1.1, 0.1),\n",
        "          np.arange(0, 1.1, 0.1),\n",
        "          np.arange(0, 0.55, 0.05),\n",
        "          np.arange(0, 0.22, 0.02),\n",
        "          np.arange(0, 0.11, 0.01)]\n",
        "\n",
        "episodes = 10\n",
        "errors = [np.zeros(len(alphas_)) for alphas_ in alphas]\n",
        "\n",
        "\n",
        "for run in range(runs):\n",
        "  for lambda_index, rate in enumerate(lambdas):\n",
        "    for alpha_index, alpha in enumerate(alphas[lambda_index]):\n",
        "      value_function = TDLambda(rate, alpha)\n",
        "      for episode in range(episodes):\n",
        "        random_walk(value_function)\n",
        "        state_values = [value_function.value(state) for state in states]\n",
        "        errors[lambda_index][alpha_index] += np.sqrt(np.mean(np.power(state_values - real_state_values[1: -1], 2)))\n",
        "\n",
        "for error in errors:\n",
        "  error /= episodes * runs\n",
        "\n",
        "for i, lambd in enumerate(lambdas):\n",
        "  plt.plot(alphas[i], errors[i], label=f\"lambda = {lambd}\")\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"RMS error\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HzEWI_wIM2Em"
      },
      "id": "HzEWI_wIM2Em",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0befd094",
      "metadata": {
        "id": "0befd094"
      },
      "source": [
        "## Q2: Implement the double Q-learning algorithm on the Cliff Walking example (5%)\n",
        "\n",
        "#### Task:\n",
        "- Set the number of episodes to 1000\n",
        "- Output the estimated optimal policy\n",
        "- Plot the average reward during episodes (see the plot in Seminar 4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "\n",
        "# Initialize the environment\n",
        "env = gym.make('CliffWalking-v0')\n",
        "\n",
        "# Define the state, action, and reward spaces\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "reward_space = np.zeros((num_states, num_actions))\n",
        "for state in range(num_states):\n",
        "    for action in range(num_actions):\n",
        "        for next_state, reward, done, info in env.P[state][action]:\n",
        "            reward_space[state, action] += reward\n",
        "\n",
        "# Initialize the Q tables\n",
        "Q1 = np.zeros((num_states, num_actions))\n",
        "Q2 = np.zeros((num_states, num_actions))\n",
        "\n",
        "# Set the hyperparameters\n",
        "alpha = 0.5\n",
        "gamma = 1.0\n",
        "epsilon = 0.1\n",
        "num_episodes = 1000\n",
        "\n",
        "# Define the epsilon-greedy policy\n",
        "def epsilon_greedy(Q1, Q2, state, epsilon):\n",
        "    if np.random.uniform() < epsilon:\n",
        "        action = np.random.randint(num_actions)\n",
        "    else:\n",
        "        Q = (Q1 + Q2) / 2.0\n",
        "        action = np.argmax(Q[state])\n",
        "    return action\n",
        "\n",
        "# Define the update rule\n",
        "def update(Q1, Q2, state, action, reward, next_state, alpha, gamma):\n",
        "    if np.random.uniform() < 0.5:\n",
        "        Q1[state, action] += alpha * (reward + gamma * np.max(Q2[next_state]) - Q1[state, action])\n",
        "    else:\n",
        "        Q2[state, action] += alpha * (reward + gamma * np.max(Q1[next_state]) - Q2[state, action])\n",
        "    return Q1, Q2\n",
        "\n",
        "# Initialize the variables for tracking the rewards and steps\n",
        "rewards = np.zeros(num_episodes)\n",
        "steps = np.zeros(num_episodes)\n",
        "\n",
        "# Loop over the episodes\n",
        "for episode in range(num_episodes):\n",
        "    # Reset the environment to the starting state\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    t = 0\n",
        "    # Loop over the steps in the episode\n",
        "    while not done:\n",
        "        # Select an action according to an epsilon-greedy policy based on the average of the two Q tables\n",
        "        action = epsilon_greedy(Q1, Q2, state, epsilon)\n",
        "        # Take a step\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        # Update the selected Q table with the observed reward and next state, and update the average Q table accordingly\n",
        "        Q1, Q2 = update(Q1, Q2, state, action, reward, next_state, alpha, gamma)\n",
        "        # Update the variables for tracking the rewards and steps\n",
        "        rewards[episode] += reward\n",
        "        steps[episode] += 1\n",
        "        # Update the state and time step\n",
        "        state = next_state\n",
        "        t += 1\n",
        "\n",
        "# Output the estimated optimal policy based on the average of the two Q tables\n",
        "Q = (Q1 + Q2) / 2.0\n",
        "policy = np.argmax(Q, axis=1)\n",
        "policy_str = \"\"\n",
        "for state in range(num_states):\n",
        "    if state == env.reset(): # starting state\n",
        "        policy_str += \"S\"\n",
        "    elif reward_space[state, :].max() == 0: # cliff state\n",
        "        policy_str += \"X\"\n",
        "    elif reward_space[state, :].max() < 0: # off-cliff state\n",
        "        policy_str += \"-\"\n",
        "    elif policy[state] == 0:\n",
        "        policy_str += \"U\"\n",
        "    elif policy[state] == 1:\n",
        "        policy_str += \"D\"\n",
        "    elif policy[state] == 2:\n",
        "        policy_str += \"L\"\n",
        "    elif policy[state] == 3:\n",
        "        policy_str += \"R\"\n",
        "\n",
        "\n",
        "print(\"Estimated optimal policy:\")\n",
        "print(policy_str[:12])\n",
        "print(policy_str[12:24])\n",
        "print(policy_str[24:36])\n",
        "print(policy_str[36:48])\n",
        "\n",
        "#Plot the average reward during episodes\n",
        "plt.plot(np.arange(num_episodes), rewards)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Average reward\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HDWRmJxGNIsR",
        "outputId": "28999d46-d048-48e2-bc9e-42274918f616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "id": "HDWRmJxGNIsR",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated optimal policy:\n",
            "DDLLUDDDDDDL\n",
            "DLDDDDDDDDLL\n",
            "DDDDDDDDDDDL\n",
            "SUUUUUUUUUUU\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEGCAYAAACgt3iRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZwcdZ3/8ddneu6ZzJFM7sl9ALkISYSEhDNBghzhUkEEVpBjhVXXn4tBQFBkdb1wUQSziuIqIh4gsigQRFhUjiAsZwLDEUgIIQc5STLX5/dHVc/UzHTPdE9Pz/l+Ph7zmO5vVXV9q6u6PvU96lvm7oiIiGQip6czICIifZ+CiYiIZEzBREREMqZgIiIiGVMwERGRjOX2dAZ6SlVVlY8fP76nsyEi0qc89dRTm919aOv0ARtMxo8fz6pVq3o6GyIifYqZrU2UrmouERHJmIKJiIhkTMFEREQypmAiIiIZ6zfBxMyWmtkaM6sxs+U9nR8RkYGkXwQTM4sBNwLHAdOAM81sWs/mSkRk4OgXwQQ4GKhx99fcvRa4HVjWw3kSERkw+st9JqOBtyLv1wGHtJ7JzC4ELgQYO3Zs9+QMqGtoZE9dA4MKcuP5AMDdeXbddt7etoet79cyfVQ5G7bt4biZI6lraGTTzn0889Y2Jg8rpSgvxuiKIhrdyTGjrrGRO/+xntPmVrNp5z4cGFlWyN9e3UKjOw+8uJGpIwaxY08doyoKqa4spiA3h5c27GDr7jpe3LCDYYMK2LB9D4NL8tlvRBk79tTxysadjB1cTH2jYwa19Y3UNTivb97NkmnDKS2IUd/gDC8rpDAvxtotu3l1027mjK2g0Z37X9zIqQdVs3brbqYOH8SWXbU8tXYru/bVM2dsJTXv7qLRndxYDrNGlwOweXctDQ2N1Dc6771fy7r39lBZnE9BXg5lhXnUvLuLHDNyDIrzY8weW8GbW/ZQ19BIVWk+2/bUsaeugbLCPMxgdEURj7y8mWFlBdTWN1KSH2PaqDJefHsHjQ4vb9xJdWUx44YU83/rtnH6nGoee20LBXkxtuyq5d2de1k0uYpX3t3F4JJ8duypw8zYuGMvpQW51NY3Mrg0nxyD/FgMx3l+/XYmDS3l7e17mTqslLqGRkZWFLF1dy1rt+xmd20Dc8dWsqeugbyYMWdsJSMrihhaWsCjNZt4e9teXtm4k/1HBvthZnU5L7+zk0GFeQBMGFpCRVEe9Y3O+m3B9/PyOzsZM7iY8VXF/Ozva2lsdF7bvJvRFUXs2lfPwRMG88rGnQwpLaCh0dmxp4783ByGDipg865atuzaR14sh/KiPMqK8thb10BRXoxGd9a8Eyx39P7DWPPODoaXFVKzaRfVlcXU1TeydstuYjk5TB1eyrPrt7Pqja2UF+VzwMhB5MdyqCjOY0hpAWu3vM+++gYaGp3Jw0opyc/l9iff5LApQ3F3tu2p4/CpQ9mwbS9b36/l3R17GVKSz6Zd+9i9r4Fjp4/gza27+d9XNjNuSDF5sRyeXbedycNKGTu4mI079rJ1dy2lhbmU5ucysqKIiUNLePL1rWzetY/cWA6FuTFmjSlnx546Xnx7B4V5MbburmXq8FLMjPXb9gBQWpDLm1vex3G276njiKnDeO/9Wt6vreegMZXUNTTy0oYdTB0xiDXv7KS0IJetu2vZta+esqI8GhqDx3lUFOexe189tfWNFOfnUtvQiIXpwfnAaWwMtn1ffQMxMypL8qmtb8SBwtwYJQUx3tm+lx176xhTGfwey4vyqNm0i5FlhWzcuZfJQ0t56709jB9SzIbte9m+p47a+kYK8nLIzcmhMC9GSX6MRoe8XGPqsEE8uXYrlcX5nHnwWMqL8rr0PGf94XkmZnY6sNTdPxm+Pxs4xN0vTbbMvHnzvCtvWnx5407Wv7eHo/YfxjNvbeOGB1/hqP2H8eq7u/jp395oMe/XT53JC2/v4L8fS3jvT1piOdZ0EB+131AeWrMp488Ukf5t5eeOYPKw0k4ta2ZPufu81un9pZprPTAm8r46TOs2H7z+ET7x0yc59GsPcvKNf+XPq9/lqruebxNIAJb/7rkuCSRAUyABEgaS4vxYwuUuPWoyt33yEKaNLGsz7bNLprD62qV8aOaILsnjlccfwB0XLeAPly7itk8ewp//3xEJ5/vC0v15+qpj+JejJzelja4oSjjvtcump7TuhZOHpDRfeVEev7xgPjedNYeC3PR+FivOnsu3P3wgj39xMXdfupD/+9IH2X/EoITztWfx/sOaXh83YwQXHTGRTywcz20XHMJxM1LfF39bfjSfXDQh4bSbPz6n6fU1JyZuVjxm2vCE6Ted1bzsj85pcy5pV6L9cMOZB6X1Ga39/Pw2lQ8Z+evyoxOmjxncfAze/PG5/OKTh3DXJQu5+IhJTelhZUNCVx5/ACs/dzj3/MsiPnfM1Kb0fzt2P27++Bw+deQk7rhoQVP6iQeOanq9dPoIVl+7lFVXLkn42QdWl7e7TdedMoMnvriYJ65Y3CI9N6edDHdSfwkmTwJTzGyCmeUDZwB3d8eKn1+/nXuefbvp/dvb93bZZ//ygvlNB0teLPnOH1FWmHTaE1cs4aTIwRn3+WP349DJVUwf1TaYxMwozItRXpQPwMzR7R+wHdlvxCAOnjCYmdXlHDq5iolDS/nuR2e3mW/i0BIqS/I5Z8H4prSq0vyEnzlmcHFK6545uqLd6fHtnzO2ggWThnDczJEsOSDxyTSZD4wfzGlzqxleVsis6grKi/MoKWhbgzy9g+/xw/Oar4e+85HZXH7cAVx94nQOnVTFhYdPbJr26cVTAKiubD7JTYlcZY6qKGLe+MFN74+Y2jyMUnTbovNEfemEaU3riBpR3nyclYVVJMkuVlo7fW51m7REx2U6PjChssX7r506s9OfVVGcx+iKIg5odXG1/Lj9qa4IjrWLDp/I0hkjWDi5itljKpgWHjsLJw9h9pjkx9khE4YwedggZowu55AJzd/5+YsmsHTGSC5buj8HR9InVpU0vZ5ZXU5hXoyq0oKEn3125LeSyJjKYoaVFTJsUMtzREzBJDF3rwcuBe4DXgLucPcXumPdJ3zvUS697el25xlUmMsPz57LirPn8tt/PrTFtGRXhwBzxlU01Zl/r52ruLKi5E1fRXmp/dijWl9lpXrCSCbR8kWJ0sK8thc4mz8ztea+WAdH+JDwR1oY+Z4K0/zOYgnym6j6uKN9Ef1OWpeOonnKD9dXEvkOWn/H0ffR80b0JJKszjzRvgFaBMh0j4nOHIcdyW+1cwcVdr4JuDA3cf6M5m3Na7W+4nCbOmopKMrPibxuXk+y0kF0v3T0W+joe022n7IRTPpLAzzufi9wbzeti9/+Yz0nzBqZ0vyThpZy7PSgmqLm3V0tprW+EorKy8mhMTxSi9o5eZYVJm9Ii+VYu0XwRFr/OHLS/YBWEp2cEx3k8bTcjiJAkuUT6SjvVSX5beZL90QZS/H76ehzCyMBJKfVjz36HcanRU9MrQNAsoBgKWxnsvToiSvd7yjdAJ0Ka/W9ZxKw2ju3xvOe2+rEXpCX2rV4UZKgn+yEXtRinvbXUdhBHpIdB9mo5uo3waS7vPD2dn69ah0//dsbbNyRWpVWdL+1LkUk29kQnDTiJ/a8dnZ+Rwd15w+brumckehH3l5pJZWSSXvfW1SHwWRQ2+qDtINJgn2T6JvrqC2mvW2KnjTix0T0e239HadyYk1Wukt2lR7NX6olw2TzZ+PKONVjIpHWgSkqHkRal0ws8stqr3RSlKTUm2ydluR1Iq0vOlpLtp86Wq4zFEzS9K371jQ1dKd6wR49obUuRXR0xebx01I767IODrn2fijdIdGPPNFB3lTN1cHVWLB8aieOjja9srhtm0za1Vwp/jA72g8FSU7ikPgEH/2aWuc5lRNrsuCW7EQT/c7TLQVko5qrK9fRXq/WeBBJ5SInkZYluu495SYrgagBvpcZWZ684Tsq+uNs/aPvaKeODRua26vKai2WY4wfkryB+qj9mhtkzzh4TJvp8YbFkeVBA+8pB41Oed2J5CYIDomCQbyuOPp9JT2x5TX/KFvXnccNL0vcaAnN+2502Ig9vqr5++qKaq7OfGepVLVAc4CMXqQMadVRIXoCO2Ri4h5t8e+2JMH2ViRoT4kGtJKC4PXlHzogeaZD8ycObhPcEn1nlcWdv+9hUEFu2ifq6PrOODjxfWf7jRjUFERaH8dTh5c2LdvedUK0VNlewDtnwTgmDi1heJLzSqIefQbt/tajx0X0fKWSSR/Verf9/pKFnH/rKjbv2oeZUV6Ux/Y9dQBUlRawede+pnmvOWk6R+03jBmjy5kztoJ/vLmtaVpVaT7lRXkcOKacR2s2A/D4FxdTmBujIC+H+rDb8OwxFdz59Hr2Gz6INRt38qNzP9D0GXPHDebAMRX831vB50b7n//zkZMYN6SYkw4cxZxxlSz5zsMcNqWKXfvqycvJ4Yk3tjKrupyGRueFt3dw6kGj+d3TLXtklxXmJjxJRE8u3zhtFmbNjeFR3/3obFY88hrLZo9meFkBR3zzL03L/+QTH2DHnjoOrK7g1Jv+xtbdtU3LLZs9in87dj9+89S6Fp83Z2wFn1kylUWTq9i5t46ywjwGFeYyZ2xzz6DoFfttnzyE59/ezs0Pv8bW3bV8ZvEU/vPBV1p8ZqIf5tnzxzGkpIBr/vACB1aXN/WO+sOlizjx+48Cwf7bvKs5z7mxHG4972C27t7X5vOieYpfRE8bWcbM0eVMH1XOgklDOGhMZVP1WjQgXnjYRL7+x9VN77968oym7+r/rv4g7s7srzwANAfBsxeM48+r3206rm46aw45Ocby4/YnL5ZDbiyHN75+PABX3fU8ACs/dzhLvvNIkHbCNHJzgmP7xANHtfkN/Oafg66w/7pkKtevfBmAuy9dxM8fX8vIskIeXP0u//vK5jbfw0OfP5Kf/f2Npu7Lq65cwisbdzFpaAl76xrbzP/z8w9hX30D59/a8p6yv19+NI0eHJ/5uTlNFyQfO2QsV931PHddspBGd+aMreTPq98F2gb7YWWFTd/BT/76ept1x0VLpO21cXxl2Yym15OHlbZpX73+o7N5ddOjFObFeHbddiC44Lvn04ex+Nt/YeOOfQwbVMBhU4ZyykGjWTSlqsXyf/iXRcz76kpAbSa9QvTAaGh77CZ08ZGTWrw/cEwFv754Abc/+SbjhxRzz78s4tv3r+Hzx+5HdWUx45f/T9O8xfm5HDczaOj/1UULaGh06hoaWbvlfWaEXU1/8JcaAC46YiLDE3QTPmfBOBZMGsLU4W3vfWgteiNTXiyHZbNHN6XHfzgAN/3lVZ54YysLJg5pcXXaOpg8e82xCdcTvYr8yAfalo7ixg0p4bpT2nb5zM/N4aj9mu/L+PXFC1j87YepKM7jqSuPSVr1ZGZNXWUrwiqu6OdAcweAMw8ey6GTqzh0chWbd9Wy4pHX0qheM46fNZLjW3XSmFld3uJ7XPniRjbt2kejOxOqSpgQ6RYalZNjlOTH+PTiKU0XCWbGZUv3b5rntEj320QN9nEfnz+u6XW8NPjG149nw/Y9TaXRvFgO5y0az6M1m5lYVdJ0DEbvrWgtWno+P8F9LqMrili/bQ//cdpMZlUHXWk/NHME1698maGDChgzuJjLjwuOpU279jUFk2gAn1BVwtUnNt9jVFVa0NRtdtPOlkH4xo/NYdGUKvbWNbTJy6DCPEoTdN8+e/44zo58P9BcIqlvzLwNMdUq58OmVLUJJoV5Me7/1yP46j0v8uy67XxkXjX7hfczDR1UwMYd+/jh2XM5aGxloo9s0b040041iSiYZKCxnYMrN8f4xumzGBpeKbQ2oaqk6YczZnAx3z2j4xu48mI55MWCg2pGGvd+mFn7gaQToyDUhZG0daNkqrq6Dj3azpJp4268WqMu1auFDCxJcoNgIi98ZSkANz5U0+G86d54Cc3VmnHxk2hjisdHontrovISNGSncvyk3msq8TGVaB3pXJnH813bDcdDOqYMa/ubTvXYV8mkF4ieYBra+ZE9ctlRjEpy93a2dNQQ35UyDSZd3ZundbfNjD4rfiXay04e6eiKThfxfZvqBXlHFwjxz4t2/c5LIei11zEhlfUnOtbSuTKPH1v1DX1/6Kk43bTYC8TbNqD9Ym9XntyyrhMnnvhVWl5u79jOzga1hJ8VnuDquqBaI5uy3UkvfkWeasmko0bd/PB7jXZzT+UKOdVSVjonyPRKJn3n4iLVC8ps9PBUMElT9IfVXjVXsh5G2ZDxWJ2dqeaqD5bpzu1sT1deacVPdr395JHtMVrjJYiuWk9ugpJJKvI7UWXXkXR6M8WDSXdeXKRbyxDfR95F94Z1hqq5MtDQbsmk+0+ynb3Y6MzdyTNGB92H27uDvyMHjCxj5ujOLx9lrf5nIv59RANU7yh/da9USybfPH1Wwp5XrQ0JRxuIlgri7RwHtxonLDqWVGfafwBmdTAIYqrinVri+e8O8R6QpSkOEROv4stGw3qqFEwy0OjOtJFlvLhhB189eQZXhl0kIbMGrtwc61TPkc5eQX73jNks+Nqf01rmlINGM2dsJeNb9T765QXzKS3I5bHXtrBwclWSpQN//MxhSaedOmc0E4a07dn0+BcXJzy5VRTn8clFEzh1TtsBBdN1+NShXHTERD65qHlwxU8dOZktu2s5a/44du6tp7qyiOW/ey7jdXXGoslVfPO+NS3uF0pkcEl+070Ff/zMYWkfH3kplkw+PG9Mi0EqEw0eCsGjF37wl1c5aGzzoIhlhXn86bOHMb7Vvv74/HHc/PCrbNi+t92BTFubO66SueMq+WIK97+k6tSDRpMXM46fmXz4pBmjynk60m3/5NmjuOuZtxP2fvvSCdPYV99+qfeiIyZRUZzHaSkezzeceRC3Pf5m0u++OyiYpCla/KxvDAqVSw4Y3mLI8S+dMC2jsYieufqDKddTd4WR5UVce/KMFmNDdcTM2gQSgAWTghvkZmZ4Vfidj7QdVRhI2PU5np8rT0g+aObCyUP4a82WlNYdy7GmnnZx5cV5fOvDBwLBiMtAjwWTA8dUtOhenMw/rjqm6XVnSpDNDfCpH4urr12atMpxWFkh15zU9tEB+49IMHJ1jvHwvx3FozWbWoyo25HWA6l2hZwca+oin8xVJ0zjlDnBBdbLG3cydfigpD00z0vyeICo/NycpCMCJ9oboyqKmo7LntI7Krz7kGidZEOj4x48kTBar5vKwdKe0oLctO54jw8r3t6VU0fOnj+uxdWlSLx0nU4huTAv1mWdIfJzczh6/+E9PhxQKvJzc5pufE3lfq7+SCWTDMQb4I2u7U2Urv1GDErpSnUg6wcPFO12zRdI+vKkYyqZZKC+0XEPGr2y0eNEpCfFOlEykYFLZ8AMNLrTGK/m6iVdZCWxPlBT0uvEB2PszvY76bt0BsxAQ9gA37rNRHofnQ/TF795M9l4YSJROgNmoMHDBnisR9tMpGOV4T0CXdl1MtHQ7f1JaUEuP/nEB/hxZJTpnjQ0wYPMUnXmwWMoyY9lZRiR7rYo7HI/Z1zy584nk82uw2qAT1O0a3BDQ9i3K1IyUXVK7zRpaCl3fupQpo/qmhvZAJ64Ykm747P1B61HVO4pKz93OINLOh9MvnbqLL6ybAbb99S1GBKpLzpq/2G88OVjOxxYM5G7LlnY7s3WmVAwSVOLrsHuEDbAd/YpbNJ9kg3N3Vmd+TFL50xOMEJuuvJiOS2GrO/LOnvsxUcezwb9GjLQ2Bg2wBM0wBfnx1J68pyISH+jYJKB+kgDvJnxYvi8CRGRgUatxhlo9OA+E1VwichAp2CSphYN8I2O431iuAcRkWxSMElTtAF+9Ts7g5KJYomIDHC9LpiY2TVmtt7Mngn/PhSZdrmZ1ZjZGjM7NpK+NEyrMbPl3ZHPyuI8Xt4YBhNVdPVan1g4geNnjeT8hZkNviki7eutDfDXu/u3oglmNg04A5gOjAJWmtnUcPKNwDHAOuBJM7vb3V/MZgbLi/J47/061m/bo5JJL1ZelMeNH5vT09kQ6fd6azBJZBlwu7vvA143sxrg4HBajbu/BmBmt4fzZjWYlBU1DxGvWCIiA12vq+YKXWpmz5rZLWYWv9NsNPBWZJ51YVqy9KyKPm9EJRMRGeh6JJiY2Uozez7B3zLgJmASMBvYAHy7C9d7oZmtMrNVmzZtyuizyoqaC3U9+dxlEZHeoEequdx9SSrzmdl/AfeEb9cD0UcBVodptJPeer0rgBUA8+bNy2iAmsLc5jEJFEtEZKDrddVcZhZ99uwpwPPh67uBM8yswMwmAFOAJ4AngSlmNsHM8gka6e/Odj5bjj6qaCIiA1tvbID/hpnNJnhW6BvARQDu/oKZ3UHQsF4PXOLuDQBmdilwHxADbnH3F7KdyWgwUclERAa6XhdM3P3sdqZdB1yXIP1e4N5s5qu1nGgw6c4Vi4j0Qr2umquviEWKI2qAF5GBTsGkk1TNJSLSTMGkk2Kq5hIRaaJg0knRYNK/H9wqItIxBZNOiraT1GfpmcoiIn2FgkknxSLfnLuCiYgMbAomnRTLaf7qGlQyEZEBTsGkk2Kq5hIRaaJg0knRaq5GBRMRGeAUTDopege8SiYiMtApmHRStJpL7e8iMtApmHRS9D4TNcCLyECnYNJJMVVziYg0UTDppGgwaVQ9l4gMcAomnRS9A17VXCIy0CmYdJJKJiIizRRMOkkN8CIizRRMOimmai4RkSYKJp2kkomISDMFk06K3gHfoDYTERngFEw6KVrNpbG5RGSgUzDppEjBRCUTERnwFEw6yaLBpLHn8iEi0hvkJptgZs/RzuPN3X1WVnLUBzU0KpqIyMCWNJgAJ4T/Lwn//3f4/6zsZacvMfJjOdQ2NKo3l4gMeEmrudx9rbuvBY5x98vc/bnwbznwwe7LYu/1x88eBoBiiYgMdKm0mZiZLYy8OTTF5dr7wA+b2Qtm1mhm81pNu9zMasxsjZkdG0lfGqbVmNnySPoEM3s8TP+VmeVnkrfUt6G5R5dKJiIy0KUSFM4DfmBmb5jZG8APwrRMPA+cCjwSTTSzacAZwHRgabjemJnFgBuB44BpwJnhvAD/AVzv7pOB94DzM8xbyuI3LiqYiMhA116bCeFJ/Ah3P9DMygHcfXumK3X3l8LPbz1pGXC7u+8DXjezGuDgcFqNu78WLnc7sMzMXgKOBj4WznMrcA1wU6Z5TEU8mGigRxEZ6Notmbh7A3Bm+Hp7VwSSDowG3oq8XxemJUsfAmxz9/pW6QmZ2YVmtsrMVm3atCmjjBpQVVrAiLJCrj5xWofzi4j0Z+2WTEJ/NbPvA78CdscT3f0f7S1kZiuBEQkmXeHuv08rl13E3VcAKwDmzZuXcXEiPzeHx764OON8iYj0dakEk9nh/69E0pygeikpd1/SifysB8ZE3leHaSRJ3wJUmFluWDqJzp9VCaroREQGrA6Dibsf1R0ZCd0N3GZm3wFGAVOAJwhqlaaY2QSCYHEG8DF3dzN7CDgduB04F8hqqUfNIyIibaVSMsHMjifoYVUYT3P3ryRfosPPOwX4HjAU+B8ze8bdj3X3F8zsDuBFoB64JGy3wcwuBe4DYsAt7v5C+HFfAG43s68CTwM/7my+0tqG7liJiEgf0WEwMbObgWLgKOBHBKWAJzJZqbvfCdyZZNp1wHUJ0u8F7k2Q/hrNPb6yTrVbIiJtpXKfyaHufg7wnrt/GVgATM1utkREpC9JJZjsCf+/b2ajgDpgZPay1DeohCIi0iyVNpN7zKwC+CbwD4KeXP+V1Vz1YmqAFxFpK5XeXNeGL39rZvcAhd1w82Kvp5KJiEizVBrgHwUeBv4X+OtADyQKIiIibaXSZnI2sAY4DfhbOBzJ9dnNloiI9CWpVHO9bmZ7gdrw7yjggGxnrLcz3WkiItKkw5KJmb0K3AUMJ7ghcIa7L812xnorNcCLiLSVSjXXDcCbBKMHfxo418wmZTVXfYEKJiIiTToMJu7+n+7+YWAJ8BTB80JeznK+RESkD0mlN9e3gUVAKfA34EsEPbsGNBVMRESapXLT4t+Bb7j7xmxnpi9Q12ARkbZSaTP5HXCMmV0FYGZjzazbBlbsbdQALyLSVirB5EaCwR3jz1nfGaYNaHo4lohIs1SquQ5x9zlm9jSAu79nZvlZzpeIiPQhqZRM6swsRjDAI2Y2FGjMaq76AJVLRESapXqfyZ3AMDO7DngU+Pes5kpERPqUdqu5zCwHeB24DFhMcEF+sru/1A1569XUZCIi0qzdYOLujWZ2o7sfBKzupjz1agoiIiJtpVLN9aCZnWbqvgSoa7CISCKpBJOLgF8D+8xsh5ntNLMdWc5Xr6dRg0VEmqUyBP2g7siIiIj0XamUTCQBVfqJiDRTMEmTgoiISFsKJmlSA7yISFspBRMzW2RmnwhfDzWzCZms1Mw+bGYvmFmjmc2LpI83sz1m9kz4d3Nk2lwze87MaszshnjvMjMbbGYPmNkr4f/KTPKW8jZ0x0pERPqIVB7bezXwBeDyMCkP+HmG630eOBV4JMG0V919dvh3cST9JuACYEr4F3908HLgQXefAjwYvhcRkW6USsnkFOAkYDeAu78NZNTDy91fcvc1qc5vZiOBMnd/zN0d+Blwcjh5GXBr+PrWSHp2qWgiItIklWBSG57A4wM9lmQ3S0wws6fN7GEzOyxMGw2si8yzLkwDGO7uG8LX7wDDk32wmV1oZqvMbNWmTZu6POMiIgNVKkPQ32FmPwQqzOwC4DzgvzpayMxWAiMSTLrC3X+fZLENwFh332Jmc4G7zGx6CnkEwN3dzJI2kbv7CmAFwLx58zJqStdNiyIizVK5afFbZnYMsAPYD/iSuz+QwnJL0s2Mu+8D9oWvnzKzV4GpwHqgOjJrdZgGsNHMRrr7hrA67N101ysiIplJpWRCGDw6DCCZCp+VstXdG8xsIkFD+2vuvjUcymU+8DhwDvC9cLG7gXOBr4f/k5V6REQkS1LpzbUzPJFH/94yszvDE37azOwUM1tH8Djg/zGz+8JJhwPPmtkzwG+Ai919a04sALEAAA+ySURBVDjtU8CPgBrgVeCPYfrXCZ5R/wqwJHyfdbp5UUSkWSolk+8SNHjfRtCH6QxgEvAP4BbgyHRX6u53Ejxwq3X6b4HfJllmFTAjQfoWgmetiIhID0mlN9dJ7v5Dd9/p7jvCRuxj3f1XQLfcINgbqWAiItIslWDyvpl9xMxywr+PAHvDaRpcREREUgomZwFnE/SS2hi+/riZFQGXZjFvvZqeFSYi0iyVrsGvAScmmfxo12ZHRET6og6DiZkVAucD04HCeLq7n5fFfImISB+SSjXXfxPcyX4s8DDBDYM7s5mpvkC1XCIizVIJJpPd/Spgt7vfChwPHJLdbImISF+SSjCpC/9vM7MZQDkwLHtZ6htUMBERaZbKTYsrwgdOXUkwdEkpcFVWcyUiIn1Ku8HEzHKAHe7+HsGDrDo1fIqIiPRv7VZzuXsjcFk35aVPUQO8iEizVNpMVprZ581sTPi89cFmNjjrORMRkT4jlTaTj4b/L4mkOQO+yktFExGRuFTugJ/QHRkREZG+K5XnmRSb2ZVmtiJ8P8XMTsh+1no3tZmIiDRLpc3kJ0AtcGj4fj3w1azlSERE+pxUgskkd/8G4c2L7v4+ajAQEZGIVIJJbTjcvAOY2SRgX1Zz1QcomoqINEulN9c1wJ+AMWb2C2Ah8E9ZzJOIiPQxqfTmut/MngLmE1yQf8bdN2c9Z72cHo4lItIsleeZ/AG4Dbjb3XdnP0siItLXpNJm8i3gMOBFM/uNmZ0ePjBrQFO5RESkWSrVXA8DD5tZDDgauAC4BSjLct5ERKSPSKUBnrA314kEQ6vMAW7NZqZERKRvSaXN5A7gYIIeXd8HHg5HEx7Q1P4uItIslTaTHxPcuHixuz8EHGpmN2ayUjP7ppmtNrNnzexOM6uITLvczGrMbI2ZHRtJXxqm1ZjZ8kj6BDN7PEz/lZnlZ5I3ERFJX4fBxN3vA2aZ2TfM7A3gWmB1hut9AJjh7rOAl4HLAcxsGnAGMB1YCvzAzGJhe82NwHHANODMcF6A/wCud/fJwHvA+RnmLSWmJngRkSZJg4mZTTWzq81sNfA94C3A3P0od/9eJit19/vdvT58+xhQHb5eBtzu7vvc/XWghqCK7WCgxt1fc/da4HZgmQU3exwN/CZc/lbg5EzyJiIi6WuvZLKa4ER9grsvCgNIQxbycB7wx/D1aIKgFbcuTEuWPgTYFglM8XQREelG7TXAn0pQ5fSQmf2JoDSQct2Oma0ERiSYdIW7/z6c5wqgHvhFyjnOgJldCFwIMHbs2Aw/qytyJCLSPyQNJu5+F3CXmZUQVD99FhhmZjcBd7r7/e19sLsvaW+6mf0TcAKw2N09TF4PjInMVh2mkSR9C1BhZrlh6SQ6f6I8rQBWAMybN8+TzSciIulJpQF+t7vf5u4nEpysnwa+kMlKzWwpcBlwUjikfdzdwBlmVmBmE4ApwBPAk8CUsOdWPkGJ6e4wCD0EnB4ufy7w+0zyJiIi6Uula3ATd3/P3Ve4++IM1/t9YBDwgJk9Y2Y3h5//AnAH8CLBfS2XuHtDWOq4FLgPeAm4I5wXgsD2OTOrIWhD+XGGeWuXqzwjItJGSnfAd7WwG2+yadcB1yVIvxe4N0H6awS9vUREpIekVTIRNbyLiCSiYCIiIhlTMBERkYwpmKRJDfAiIm0pmIiISMYUTEREJGMKJiIikjEFkzSpa7CISFsKJmlSA7yISFsKJiIikjEFExERyZiCiYiIZEzBJE1qgBcRaUvBJE1qgBcRaUvBREREMqZgIiIiGVMwERGRjCmYiIhIxhRMREQkYwomaVLXYBGRthRM0qSuwSIibSmYiIhIxhRMREQkYwomIiKSMQUTERHJmIKJiIhkrEeCiZl908xWm9mzZnanmVWE6ePNbI+ZPRP+3RxZZq6ZPWdmNWZ2g1nQSdfMBpvZA2b2Svi/sie2SURkIOupkskDwAx3nwW8DFwemfaqu88O/y6OpN8EXABMCf+WhunLgQfdfQrwYPheRES6UY8EE3e/393rw7ePAdXtzW9mI4Eyd3/M3R34GXByOHkZcGv4+tZIuoiIdJPe0GZyHvDHyPsJZva0mT1sZoeFaaOBdZF51oVpAMPdfUP4+h1geLIVmdmFZrbKzFZt2rSpi7IvIiK52fpgM1sJjEgw6Qp3/304zxVAPfCLcNoGYKy7bzGzucBdZjY91XW6u5tZ0nvU3X0FsAJg3rx5upddRKSLZC2YuPuS9qab2T8BJwCLw6or3H0fsC98/ZSZvQpMBdbTsiqsOkwD2GhmI919Q1gd9m6XboiIiHSop3pzLQUuA05y9/cj6UPNLBa+nkjQ0P5aWI21w8zmh724zgF+Hy52N3Bu+PrcSLqIiHSTrJVMOvB9oAB4IOzh+1jYc+tw4CtmVgc0Ahe7+9ZwmU8BPwWKCNpY4u0sXwfuMLPzgbXAR7prI0REJNAjwcTdJydJ/y3w2yTTVgEzEqRvARZ3aQZFRCQtvaE3l4iI9HEKJiIikjEFExERyZiCiYiIZEzBREREMqZgIiIiGVMwERGRjCmYiIhIxhRMREQkYwomIiKSMQUTERHJmIKJiIhkTMFEREQypmAiIiIZUzAREZGMKZiIiEjGFExERCRjCiYiIpIxBRMREcmYgomIiGRMwSRNhXnBV5Zj1sM5ERHpPXJ7OgN9zY1nzeH2J97igJGDejorIiK9hoJJmkaWF/Gvx0zt6WyIiPQqquYSEZGMKZiIiEjGeiyYmNm1ZvasmT1jZveb2agw3czsBjOrCafPiSxzrpm9Ev6dG0mfa2bPhcvcYKbWcRGR7tSTJZNvuvssd58N3AN8KUw/DpgS/l0I3ARgZoOBq4FDgIOBq82sMlzmJuCCyHJLu2sjRESkB4OJu++IvC0BPHy9DPiZBx4DKsxsJHAs8IC7b3X394AHgKXhtDJ3f8zdHfgZcHL3bYmIiPRoby4zuw44B9gOHBUmjwbeisy2LkxrL31dgvRE67uQoLTD2LFjM98AEREBslwyMbOVZvZ8gr9lAO5+hbuPAX4BXJrNvITrW+Hu89x93tChQ7O9OhGRASOrJRN3X5LirL8A7iVoE1kPjIlMqw7T1gNHtkr/S5henWB+ERHpJj1WzWVmU9z9lfDtMmB1+Ppu4FIzu52gsX27u28ws/uAf480un8QuNzdt5rZDjObDzxOUG32vY7W/9RTT202s7WdzH4VsLmTy/ZV2uaBQds8MGSyzeMSJfZkm8nXzWw/oBFYC1wcpt8LfAioAd4HPgEQBo1rgSfD+b7i7lvD158CfgoUAX8M/9rl7p2u5zKzVe4+r7PL90Xa5oFB2zwwZGObeyyYuPtpSdIduCTJtFuAWxKkrwJmdGkGRUQkZboDXkREMqZg0jkrejoDPUDbPDBomweGLt9mC2qVREREOk8lExERyZiCiYiIZEzBJE1mttTM1oQjFC/v6fx0BTMbY2YPmdmLZvaCmX0mTB9sZg+EozQ/EL/Hp72RnfsaM4uZ2dNmdk/4foKZPR5u26/MLD9MLwjf14TTx/dkvjvLzCrM7DdmttrMXjKzBf19P5vZv4bH9fNm9kszK+xv+9nMbjGzd83s+Uha2vs12cjsqVAwSYOZxYAbCUY2ngacaWbTejZXXaIe+H/uPg2YD1wSbtdy4EF3nwI8GL6HJCM791GfAV6KvP8P4Hp3nwy8B5wfpp8PvBemXx/O1xf9J/And98fOJBg2/vtfjaz0cCngXnuPgOIAWfQ//bzT2k7Wnpa+7WDkdk75u76S/EPWADcF3l/OcFd+D2ety7ezt8DxwBrgJFh2khgTfj6h8CZkfmb5utLfwRD7zwIHE3wGAQjuCs4t/X+Bu4DFoSvc8P5rKe3Ic3tLQdeb53v/ryfaR4gdnC43+4hGIG83+1nYDzwfGf3K3Am8MNIeov5OvpTySQ9yUYu7jfCYv1BBEPTDHf3DeGkd4Dh4ev+8j18F7iMYBQGgCHANnevD99Ht6tpm8Pp28P5+5IJwCbgJ2HV3o/MrIR+vJ/dfT3wLeBNYAPBfnuK/r2f49LdrxntbwUTaWJmpcBvgc96y+fN4MGlSr/pR25mJwDvuvtTPZ2XbpQLzAFucveDgN00V30A/XI/VxKM/TcBGEXw7KQB9/C87tivCibpSTaicZ9nZnkEgeQX7v67MHmjBQ8fI/z/bpjeH76HhcBJZvYGcDtBVdd/EjyMLT7MUHS7mrY5nF4ObOnODHeBdcA6d388fP8bguDSn/fzEuB1d9/k7nXA7wj2fX/ez3Hp7teM9reCSXqeBKaEPUHyCRry7u7hPGXMzAz4MfCSu38nMuluIN6j41yCtpR4+jlhr5D5hCM7d1uGu4C7X+7u1e4+nmA//tndzwIeAk4PZ2u9zfHv4vRw/j51Be/u7wBvWTDAKsBi4EX68X4mqN6ab2bF4XEe3+Z+u58j0t2v9wEfNLPKsET3wTAtNT3daNTX/ghGNH4ZeBW4oqfz00XbtIigCPws8Ez49yGCuuIHgVeAlcDgcH4j6NX2KvAcQU+ZHt+ODLb/SOCe8PVE4AmCUat/DRSE6YXh+5pw+sSezncnt3U2sCrc13cBlf19PwNfJnjExfPAfwMF/W0/A78kaBOqIyiBnt+Z/QqcF257DfCJdPKg4VRERCRjquYSEZGMKZiIiEjGFExERCRjCiYiIpIxBRMREcmYgolIFzGzBjN7JvLX7qjSZnaxmZ3TBet9w8yqMv0ckUyoa7BIFzGzXe5e2gPrfYPgXoHN3b1ukTiVTESyLCw5fMPMnjOzJ8xscph+jZl9Pnz9aQueJ/Osmd0epg02s7vCtMfMbFaYPsTM7g+f0fEjgpvQ4uv6eLiOZ8zsh+FjE0SyTsFEpOsUtarm+mhk2nZ3nwl8n2C04taWAwe5+yzg4jDty8DTYdoXgZ+F6VcDj7r7dOBOYCyAmR0AfBRY6O6zgQbgrK7dRJHEcjueRURStCc8iSfyy8j/6xNMfxb4hZndRTDMCQTD3JwG4O5/DkskZcDhwKlh+v+Y2Xvh/IuBucCTwTBUFNE8uJ9IVimYiHQPT/I67niCIHEicIWZzezEOgy41d0v78SyIhlRNZdI9/ho5P/foxPMLAcY4+4PAV8gGPa8FPhfwmoqMzsS2OzBc2YeAT4Wph9HMFgjBIP6nW5mw8Jpg81sXBa3SaSJSiYiXafIzJ6JvP+Tu8e7B1ea2bPAPoLHo0bFgJ+bWTlB6eIGd99mZtcAt4TLvU/zcOJfBn5pZi8AfyMYZh13f9HMrgTuDwNUHXAJsLarN1SkNXUNFskydd2VgUDVXCIikjGVTEREJGMqmYiISMYUTEREJGMKJiIikjEFExERyZiCiYiIZOz/A7QcGimPa9zjAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}